{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f904acd",
   "metadata": {},
   "source": [
    "### 모델 학습용 코드 구현 및 실행\n",
    "\n",
    "- 학습별 코드 분리 (구분선 사용 및 해당 모델 이름 작성)\n",
    "- 학습된 파라미터는 ./parameters 에 .pth 형식으로 저장하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62e61870",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = ['./data/2022Data_part1.csv', './data/2022Data_part2.csv']\n",
    "\n",
    "### Colab 사용시 주석 제거\n",
    "\n",
    "# !rm -rf SKN19_2ND_5TEAM\n",
    "# !git clone https://github.com/SKNetworks-AI19-250818/SKN19_2ND_5TEAM.git\n",
    "# %cd SKN19_2ND_5TEAM\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/SKN19_2ND_5TEAM')\n",
    "# input_file_path = ['/content/SKN19_2ND_5TEAM/data/encoded_dataset.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6121bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import modules.DataAnalysis as DataAnalysis\n",
    "import modules.ModelAnalysis as ModelAnalysis\n",
    "import modules.DataModify as DataModify\n",
    "from modules.DataSelect import DataPreprocessing\n",
    "\n",
    "import modules.Models as Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9614b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d425e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SKN_19\\SKN19_2ND_5TEAM\\modules\\DataModify.py:501: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 시드 고정 : 결과 비교용\n",
    "Models.set_seed(42)\n",
    "\n",
    "dp = DataPreprocessing()\n",
    "\n",
    "# device 설정 (cuda 사용 가능 시 cuda 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset 로드\n",
    "dataset = DataModify.CancerDataset(\n",
    "    target_column='target_label',              # target column\n",
    "    time_column='Survival months_bin_3m',      # Survival months\n",
    "    file_paths=input_file_path,\n",
    "    transform=dp.run                           # 기존에 정제가 완료된 데이터를 사용할 경우 None\n",
    ")\n",
    "\n",
    "sui_input_file_path = ['./data/Suicide.csv']\n",
    "sui_dataset = DataModify.CancerDataset(\n",
    "    target_column='target_label',              # target column\n",
    "    time_column='Survival months_bin_3m',      # Survival months\n",
    "    file_paths=sui_input_file_path,\n",
    "    transform=dp.run                           # 기존에 정제가 완료된 데이터를 사용할 경우 None\n",
    ")\n",
    "\n",
    "dp.save_category()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82b1b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set size 설정 및 분리\n",
    "# 전체 길이\n",
    "n = len(dataset)\n",
    "\n",
    "# 비율 설정\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# 각 세트 크기 계산\n",
    "train_size = int(n * train_ratio)\n",
    "val_size = int(n * val_ratio)\n",
    "test_size = n - train_size - val_size  # 합이 정확히 맞도록 조정\n",
    "\n",
    "# 분리 수행\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_dataset = ConcatDataset([train_dataset, sui_dataset])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# 데이터를 로드\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = dataset.data.shape[1]   # input dimension : data의 feature의 개수\n",
    "hidden_size = (128, 64)             # 1번째, 2번째 hidden layer의 size\n",
    "time_bins = 91                     # 3개월 단위로 time을 split하여 각 구간으로 삼음 -> 270개월+ 는 하나로 취급\n",
    "num_events = 4                      # 사건의 개수\n",
    "\n",
    "# 모델 선언\n",
    "model = Models.DeepHitSurvWithSEBlockAnd2DCNN(input_dim, hidden_size, time_bins, num_events, dropout=.2).to(device)\n",
    "\n",
    "# 손실함수 및 optimizer 선언\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccf2ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422453\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c767b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save = ModelAnalysis.dataset_to_dataframe(test_dataset)\n",
    "\n",
    "df_save.to_csv(\"./data/test dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52867e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "def train_epoch(model, loader, optimizer, device=device):\n",
    "    # 모델을 train 모드로 설정\n",
    "    model.train()\n",
    "    # loss 변수 선언\n",
    "    total_loss, total_lik, total_rank = 0, 0, 0\n",
    "\n",
    "    # loader에서 불러온 데이터를 기반으로 학습\n",
    "    for x, times, events in loader:\n",
    "        x, times, events = x.to(device), times.to(device), events.to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, pmf, cif = model(x)\n",
    "        loss, L_lik, L_rank = Models.deephit_loss(pmf, cif, times, events)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_lik += L_lik.item() * x.size(0)\n",
    "        total_rank += L_rank.item() * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss/n, total_lik/n, total_rank/n\n",
    "\n",
    "# 모델 평가\n",
    "def evaluate(model, loader, device=device):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "    total_loss, total_lik, total_rank = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x, times, events = x.to(device), times.to(device), events.to(device)\n",
    "\n",
    "            logits, pmf, cif = model(x)\n",
    "            loss, L_lik, L_rank = Models.deephit_loss(pmf, cif, times, events)\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_lik += L_lik.item() * x.size(0)\n",
    "            total_rank += L_rank.item() * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss/n, total_lik/n, total_rank/n\n",
    "\n",
    "def get_cif_from_model(model, loader, device=device):\n",
    "    model.eval()\n",
    "    all_cif = []\n",
    "    all_times = []\n",
    "    all_events = []\n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x = x.to(device)\n",
    "            logits, pmf, cif = model(x)\n",
    "            all_cif.append(cif.cpu())\n",
    "            all_times.append(times)\n",
    "            all_events.append(events)\n",
    "    all_cif = torch.cat(all_cif, dim=0)  # (num_samples, num_events, time_bins)\n",
    "    all_times = torch.cat(all_times, dim=0)\n",
    "    all_events = torch.cat(all_events, dim=0)\n",
    "    return all_cif, all_times, all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fce35d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] Train Loss=0.8027 (L=0.7935, R=0.0183) | Val Loss=0.7405 (L=0.7333, R=0.0144)\n",
      "[002] Train Loss=0.7818 (L=0.7738, R=0.0160) | Val Loss=0.7159 (L=0.7082, R=0.0152)\n",
      "[003] Train Loss=0.7714 (L=0.7638, R=0.0151) | Val Loss=0.7855 (L=0.7791, R=0.0127)\n",
      "[004] Train Loss=0.7823 (L=0.7747, R=0.0151) | Val Loss=0.7031 (L=0.6967, R=0.0129)\n",
      "[005] Train Loss=0.7551 (L=0.7476, R=0.0150) | Val Loss=0.6950 (L=0.6881, R=0.0137)\n",
      "[006] Train Loss=0.7572 (L=0.7497, R=0.0150) | Val Loss=0.6995 (L=0.6928, R=0.0133)\n",
      "[007] Train Loss=0.7593 (L=0.7517, R=0.0151) | Val Loss=0.6989 (L=0.6926, R=0.0125)\n",
      "[008] Train Loss=0.7544 (L=0.7470, R=0.0150) | Val Loss=0.6982 (L=0.6909, R=0.0146)\n",
      "[009] Train Loss=0.7509 (L=0.7435, R=0.0149) | Val Loss=0.6938 (L=0.6877, R=0.0123)\n",
      "[010] Train Loss=0.7514 (L=0.7439, R=0.0150) | Val Loss=0.7014 (L=0.6949, R=0.0130)\n",
      "[011] Train Loss=0.7420 (L=0.7346, R=0.0147) | Val Loss=0.7096 (L=0.7026, R=0.0139)\n",
      "[012] Train Loss=0.7536 (L=0.7462, R=0.0148) | Val Loss=0.7031 (L=0.6961, R=0.0141)\n",
      "[013] Train Loss=0.7505 (L=0.7431, R=0.0149) | Val Loss=0.6911 (L=0.6840, R=0.0142)\n",
      "[014] Train Loss=0.7348 (L=0.7274, R=0.0147) | Val Loss=0.6845 (L=0.6781, R=0.0127)\n",
      "[015] Train Loss=0.7344 (L=0.7270, R=0.0147) | Val Loss=0.6902 (L=0.6836, R=0.0131)\n",
      "[016] Train Loss=0.7306 (L=0.7233, R=0.0146) | Val Loss=0.6868 (L=0.6798, R=0.0138)\n",
      "[017] Train Loss=0.7548 (L=0.7473, R=0.0151) | Val Loss=0.7037 (L=0.6969, R=0.0136)\n",
      "[018] Train Loss=0.7594 (L=0.7519, R=0.0151) | Val Loss=0.7126 (L=0.7056, R=0.0139)\n",
      "[019] Train Loss=0.7517 (L=0.7443, R=0.0149) | Val Loss=0.7003 (L=0.6936, R=0.0134)\n",
      "[020] Train Loss=0.7507 (L=0.7433, R=0.0148) | Val Loss=0.6999 (L=0.6931, R=0.0136)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train_loss, train_lik, train_rank = train_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_lik, val_rank = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"[{epoch:03d}] \"\n",
    "          f\"Train Loss={train_loss:.4f} (L={train_lik:.4f}, R={train_rank:.4f}) | \"\n",
    "          f\"Val Loss={val_loss:.4f} (L={val_lik:.4f}, R={val_rank:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eba13781",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./parameters/deephit_model_2D_CNN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44512d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepHitSurvWithSEBlock(\n",
       "  (se_block): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=4, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4, out_features=17, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (se_block_event): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=64, out_features=16, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=16, out_features=64, bias=True)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (shared): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (heads): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=64, out_features=91, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_params_path = './parameters/deephit_model_feature.pth'\n",
    "\n",
    "input_dim = dataset.data.shape[1]   # input dimension : data의 feature의 개수\n",
    "hidden_size = (128, 64)             # 1번째, 2번째 hidden layer의 size\n",
    "time_bins = 91                      # 3개월 단위로 time을 split하여 각 구간으로 삼음 -> 최대 270개월 + 그 후\n",
    "num_events = 4                      # 사건의 개수\n",
    "\n",
    "# 모델 정의 (학습할 때 사용한 모델 클래스)\n",
    "model = Models.DeepHitSurvWithSEBlock(input_dim, \n",
    "                    hidden_size, \n",
    "                    time_bins, \n",
    "                    num_events,\n",
    "                    )  # 사건 수 맞게 설정\n",
    "model.load_state_dict(torch.load(input_params_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # 평가 모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b26f5407",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train set CIF 추출\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cif_train, times_train, events_train \u001b[38;5;241m=\u001b[39m \u001b[43mget_cif_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 55\u001b[0m, in \u001b[0;36mget_cif_from_model\u001b[1;34m(model, loader, device)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, times, events \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 55\u001b[0m     logits, pmf, cif \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     all_cif\u001b[38;5;241m.\u001b[39mappend(cif\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m     57\u001b[0m     all_times\u001b[38;5;241m.\u001b[39mappend(times)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\SKN_19\\SKN19_2ND_5TEAM\\modules\\Models.py:143\u001b[0m, in \u001b[0;36mDeepHitSurvWithSEBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 143\u001b[0m     scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mse_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# SEBlock 수행\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     x_scaled \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m scale            \u001b[38;5;66;03m# x를 스케일링\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared(x_scaled)       \u001b[38;5;66;03m# 공유 브랜치 통과\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train set CIF 추출\n",
    "cif_train, times_train, events_train = get_cif_from_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d510be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_score_sigmoid(pmf, time_lambda=0.05, event_weights=None):\n",
    "    \"\"\"\n",
    "    pmf: torch.Tensor, shape (B, E, T) - 사건별 시간 확률\n",
    "    time_lambda: float, 지수 감쇠 계수 (시간대 가중치)\n",
    "    event_weights: list or torch.Tensor, 길이 E, 사건별 가중치\n",
    "    \"\"\"\n",
    "    B, E, T = pmf.shape\n",
    "    device = pmf.device\n",
    "\n",
    "    # 시간 가중치\n",
    "    time_weights = torch.exp(-time_lambda * torch.arange(T, device=device))\n",
    "    \n",
    "    # 사건 가중치\n",
    "    if event_weights is None:\n",
    "        event_weights = torch.ones(E, device=device)\n",
    "    else:\n",
    "        event_weights = torch.tensor(event_weights, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # 가중치 적용\n",
    "    weighted_pmf = pmf * time_weights.view(1, 1, T)\n",
    "    weighted_pmf = weighted_pmf * event_weights.view(1, E, 1)\n",
    "\n",
    "    # 가중합 계산\n",
    "    risk_score_raw = weighted_pmf.sum(dim=(1, 2))\n",
    "\n",
    "    # 0 기준으로 offset 제거 → 음수도 나오게\n",
    "    risk_score_raw = risk_score_raw - risk_score_raw.mean()\n",
    "\n",
    "    # 시그모이드 + 0~100 스케일\n",
    "    risk_score = torch.sigmoid(risk_score_raw) * 100\n",
    "\n",
    "    return risk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48265bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에서 PMF 추출\n",
    "def get_pmf_from_model(model, loader, device=device):\n",
    "    model.eval()\n",
    "    all_pmf = []\n",
    "    all_times = []\n",
    "    all_events = []\n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x = x.to(device)\n",
    "            logits, pmf, _ = model(x)  # CIF는 필요 없음\n",
    "\n",
    "            pmf = pmf[:, :, :91]  # (batch_size, num_events, time_bins-1)\n",
    "            \n",
    "            all_pmf.append(pmf.cpu())\n",
    "            all_times.append(times)\n",
    "            all_events.append(events)\n",
    "    all_pmf = torch.cat(all_pmf, dim=0)  # (num_samples, num_events, time_bins)\n",
    "    all_times = torch.cat(all_times, dim=0)\n",
    "    all_events = torch.cat(all_events, dim=0)\n",
    "    return all_pmf, all_times, all_events\n",
    " \n",
    "# train set PMF 추출\n",
    "pmf_train, times_train, events_train = get_pmf_from_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147eb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값: 99.41755\n",
      "최소값: 38.345165\n",
      "평균값: 49.816166\n",
      "앞 10개 값: [55.928837 51.859367 38.469154 45.206867 51.962406 71.034904 40.639694\n",
      " 41.435307 41.39525  58.480625]\n",
      "=== 라벨별 Risk Score 통계 ===\n",
      "\n",
      "Event -1:\n",
      "  개수: 366123\n",
      "  최대값: 98.9653\n",
      "  최소값: 38.3452\n",
      "  평균값: 48.3918\n",
      "\n",
      "Event 0:\n",
      "  개수: 41868\n",
      "  최대값: 98.0343\n",
      "  최소값: 38.4692\n",
      "  평균값: 61.6722\n",
      "\n",
      "Event 1:\n",
      "  개수: 5857\n",
      "  최대값: 98.0431\n",
      "  최소값: 38.4692\n",
      "  평균값: 52.0595\n",
      "\n",
      "Event 2:\n",
      "  개수: 6164\n",
      "  최대값: 97.9004\n",
      "  최소값: 38.4692\n",
      "  평균값: 51.0170\n",
      "\n",
      "Event 3:\n",
      "  개수: 2441\n",
      "  최대값: 99.4175\n",
      "  최소값: 38.4692\n",
      "  평균값: 51.6823\n"
     ]
    }
   ],
   "source": [
    "# 사건별 가중치 설정\n",
    "event_weights = [2.0, 3.0, 3.0, 7.0]  # 예시\n",
    "\n",
    "# 위험 점수 계산 (시그모이드 + 0~100)\n",
    "risk_scores = compute_risk_score_sigmoid(pmf_train, time_lambda=0.05, event_weights=event_weights).numpy()\n",
    "\n",
    "# 통계 확인\n",
    "print(\"최대값:\", np.max(risk_scores))\n",
    "print(\"최소값:\", np.min(risk_scores))\n",
    "print(\"평균값:\", np.mean(risk_scores))\n",
    "print(\"앞 10개 값:\", risk_scores[:10])\n",
    "\n",
    "# 사건별 통계\n",
    "events_np = events_train.numpy()\n",
    "unique_events = np.unique(events_np)\n",
    "\n",
    "print(\"=== 라벨별 Risk Score 통계 ===\")\n",
    "for e in unique_events:\n",
    "    mask = (events_np == e)\n",
    "    scores_e = risk_scores[mask]\n",
    "    if len(scores_e) == 0:\n",
    "        continue\n",
    "    print(f\"\\nEvent {e}:\")\n",
    "    print(f\"  개수: {len(scores_e)}\")\n",
    "    print(f\"  최대값: {np.max(scores_e):.4f}\")\n",
    "    print(f\"  최소값: {np.min(scores_e):.4f}\")\n",
    "    print(f\"  평균값: {np.mean(scores_e):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ed3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SKN_19\\SKN19_2ND_5TEAM\\modules\\Models.py:530: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  times = torch.tensor(times, dtype=torch.float32, device=self.device)\n",
      "d:\\SKN_19\\SKN19_2ND_5TEAM\\modules\\Models.py:531: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  events = torch.tensor(events, dtype=torch.float32, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "# 사건별 마지막 CIF를 입력으로 사용\n",
    "X_risk = cif_train[:, :, -2].numpy()  # (num_samples, num_events)\n",
    "weights = [0.3, 0.3, 1, 3]\n",
    "\n",
    "risk_target = np.zeros(X_risk.shape[0])\n",
    "for i in range(len(events_train)):\n",
    "    t_i = min(times_train[i], cif_train.shape[2]-2)  # 최대값 제한\n",
    "    if events_train[i] >= 0:\n",
    "        risk_target[i] = cif_train[i, events_train[i], t_i].item()\n",
    "    else:\n",
    "        risk_target[i] = cif_train[i, :, t_i].sum().item()  # 검열 처리\n",
    "\n",
    "risk_model = Models.WeightedCoxRiskEstimator(num_events=X_risk.shape[1], weights=weights, device=device)\n",
    "risk_model.fit(X_risk, times_train, events_train)\n",
    "\n",
    "torch.save(risk_model.event_linears.state_dict(), \"./data/parameters/risk_model_event_linears.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3468fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값: 99.9127\n",
      "최소값: 83.37278\n",
      "평균값: 91.87544\n",
      "앞 10개 값: [94.69282 93.39063 85.93979 95.19147 96.79798 94.41105 99.55224 95.50266\n",
      " 89.5379  93.62158]\n"
     ]
    }
   ],
   "source": [
    "risk_scores = risk_model.predict(X_risk)\n",
    "\n",
    "print(\"최대값:\", np.max(risk_scores))\n",
    "print(\"최소값:\", np.min(risk_scores))\n",
    "print(\"평균값:\", np.mean(risk_scores))\n",
    "print(\"앞 10개 값:\", risk_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 라벨별 Risk Score 통계 ===\n",
      "\n",
      "Event -1:\n",
      "  개수: 366123\n",
      "  최대값: 99.9127\n",
      "  최소값: 83.3728\n",
      "  평균값: 91.8788\n",
      "\n",
      "Event 0:\n",
      "  개수: 41868\n",
      "  최대값: 99.8873\n",
      "  최소값: 84.1983\n",
      "  평균값: 91.8673\n",
      "\n",
      "Event 1:\n",
      "  개수: 5857\n",
      "  최대값: 99.8445\n",
      "  최소값: 84.3004\n",
      "  평균값: 91.7705\n",
      "\n",
      "Event 2:\n",
      "  개수: 6164\n",
      "  최대값: 99.8123\n",
      "  최소값: 84.2024\n",
      "  평균값: 91.8592\n",
      "\n",
      "Event 3:\n",
      "  개수: 2441\n",
      "  최대값: 99.8139\n",
      "  최소값: 84.3451\n",
      "  평균값: 91.8066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tensor → numpy 변환\n",
    "events_np = events_train.numpy()\n",
    "\n",
    "# 사건 라벨 종류 (-1은 검열)\n",
    "unique_events = np.unique(events_np)\n",
    "\n",
    "print(\"=== 라벨별 Risk Score 통계 ===\")\n",
    "for e in unique_events:\n",
    "    mask = (events_np == e)\n",
    "    scores_e = risk_scores[mask]\n",
    "\n",
    "    if len(scores_e) == 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEvent {e}:\")\n",
    "    print(f\"  개수: {len(scores_e)}\")\n",
    "    print(f\"  최대값: {np.max(scores_e):.4f}\")\n",
    "    print(f\"  최소값: {np.min(scores_e):.4f}\")\n",
    "    print(f\"  평균값: {np.mean(scores_e):.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Team5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
